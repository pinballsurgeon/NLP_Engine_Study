{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0PMxbKszs9Jq8p/+m8e0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinballsurgeon/NLP_Engine_Study/blob/main/perception_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install squarify pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOiyLkfc7kkN",
        "outputId": "118c58aa-0225-4164-f095-7ca63532b2cf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: squarify in /usr/local/lib/python3.12/dist-packages (0.4.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2ogN02oMOJTX"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import math, os\n",
        "import squarify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference registry\n",
        "refs = pd.DataFrame([\n",
        "\n",
        "###### physical_body\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"vision_wavelength\", aspect_label=\"Vision (wavelength)\", unit=\"meters\", scale=\"log\",\n",
        "         Umin=1e-12, Umax=1e4, Hmin=380e-9, Hmax=740e-9,\n",
        "         source_universal=\"NASA EMS\", source_human=\"NASA visible\"),\n",
        "\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"vision_luminance\", aspect_label=\"Vision luminance\", unit=\"cd/m^2\", scale=\"log\",\n",
        "         Umin=1e-9, Umax=1e9, Hmin=1e-6, Hmax=1e6,\n",
        "         source_universal=\"OOM luminance\", source_human=\"Light adaptation\"),\n",
        "\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"hearing_frequency\", aspect_label=\"Hearing (frequency)\", unit=\"Hz\", scale=\"log\",\n",
        "         Umin=1e-1, Umax=1e7, Hmin=20.0, Hmax=20000.0,\n",
        "         source_universal=\"Hearing ranges\", source_human=\"NCBI human range\"),\n",
        "\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"hearing_pressure\", aspect_label=\"Hearing level (pressure)\", unit=\"Pa\", scale=\"log\",\n",
        "         Umin=1e-6, Umax=1e7, Hmin=2e-5, Hmax=2e1,\n",
        "         source_universal=\"Sound pressure\", source_human=\"SPL reference\"),\n",
        "\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"touch_vibration\", aspect_label=\"Touch vibration\", unit=\"Hz\", scale=\"log\",\n",
        "         Umin=1e-1, Umax=1e6, Hmin=2.0, Hmax=1000.0,\n",
        "         source_universal=\"Mechanical vibration\", source_human=\"Bolanowski 1988\"),\n",
        "\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"touch_spatial\", aspect_label=\"Touch spatial resolution\", unit=\"meters\", scale=\"log\",\n",
        "         Umin=1e-9, Umax=1.0, Hmin=2e-3, Hmax=4e-2,\n",
        "         source_universal=\"Contact scale\", source_human=\"Weinstein 1968\"),\n",
        "\n",
        "    dict(class_id=\"physical_body\", aspect_id=\"temporal_gap\", aspect_label=\"Temporal gap detection\", unit=\"seconds\", scale=\"log\",\n",
        "         Umin=1e-15, Umax=3.1536e9, Hmin=2e-3, Hmax=1e-1,\n",
        "         source_universal=\"Time scales\", source_human=\"Gap detection review\"),\n",
        "\n",
        "##### multisensory\n",
        "\n",
        "\n",
        "    dict(class_id=\"multisensory\", aspect_id=\"av_tbw_simple\", aspect_label=\"Audio–visual TBW (flash/beep)\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=10.0, Hmin=0.08, Hmax=0.20,\n",
        "        source_universal=\"TBW construct review (Wallace & Stevenson 2014): https://pmc.ncbi.nlm.nih.gov/articles/PMC4326640/\",\n",
        "        source_human=\"Typical adult TBW ~160 ms for simple AV (Zerr et al. 2019): https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02489/full\"),\n",
        "\n",
        "    dict(class_id=\"multisensory\", aspect_id=\"av_tbw_speech\", aspect_label=\"Audio–visual TBW (speech)\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=10.0, Hmin=0.12, Hmax=0.30,\n",
        "        source_universal=\"Intersensory synchrony tutorial (Vroomen & Keetels 2010): https://pubmed.ncbi.nlm.nih.gov/20436185/\",\n",
        "        source_human=\"Speech TBW often wider ~200–250 ms (Hillock-Dunn et al. 2016): https://www.sciencedirect.com/science/article/abs/pii/S0028393216300525\"),\n",
        "\n",
        "    dict(class_id=\"multisensory\", aspect_id=\"vh_mle_consistency\", aspect_label=\"Visual–haptic MLE consistency\", unit=\"fraction\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=1.0, Hmin=0.70, Hmax=1.00,\n",
        "        source_universal=\"MLE optimality 0–1 consistency range\",\n",
        "        source_human=\"Near-optimal integration (Ernst & Banks 2002 Nature): https://pubmed.ncbi.nlm.nih.gov/11807554/; Tool-mediated MLE (Takahashi et al. 2017): https://pmc.ncbi.nlm.nih.gov/articles/PMC5380699/\"),\n",
        "\n",
        "    dict(class_id=\"multisensory\", aspect_id=\"ventriloquism_bias\", aspect_label=\"Ventriloquism spatial bias\", unit=\"degrees\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=45.0, Hmin=1.0, Hmax=10.0,\n",
        "        source_universal=\"Ventriloquism paradigms/review (Bruns 2019): https://www.frontiersin.org/articles/10.3389/fnint.2019.00051/full\",\n",
        "        source_human=\"Near-optimal AV spatial integration; bias scales with disparity (Alais & Burr 2004): https://pubmed.ncbi.nlm.nih.gov/14761661/\"),\n",
        "\n",
        "    dict(class_id=\"multisensory\", aspect_id=\"sifi_tbw\", aspect_label=\"Sound-induced flash TBW\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.0, Hmin=0.05, Hmax=0.25,\n",
        "        source_universal=\"Task-dependent AV TBW framework (Stevenson & Wallace 2013): https://pmc.ncbi.nlm.nih.gov/articles/PMC3711231/\",\n",
        "        source_human=\"SIFI temporal window tens–hundreds of ms (Hirst et al. 2020 review): https://www.sciencedirect.com/science/article/pii/S0149763420305637\"),\n",
        "\n",
        "    dict(class_id=\"multisensory\", aspect_id=\"mcgurk_susceptibility\", aspect_label=\"McGurk susceptibility\", unit=\"fraction\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=1.0, Hmin=0.10, Hmax=0.80,\n",
        "        source_universal=\"Susceptibility expressed as 0–1 proportion\",\n",
        "        source_human=\"Large variability across adults/stimuli (Mallick et al. 2015): https://pmc.ncbi.nlm.nih.gov/articles/PMC4580505/; ranges 0–100% observed (Magnotti et al. 2024): https://www.sciencedirect.com/science/article/pii/S0010945220303749\"),\n",
        "\n",
        "######## temporal_causality\n",
        "\n",
        "    dict(class_id=\"temporal_causality\", aspect_id=\"toj_threshold\", aspect_label=\"Temporal order JND\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.0, Hmin=0.02, Hmax=0.08,\n",
        "        source_universal=\"TOJ paradigms overview: https://pmc.ncbi.nlm.nih.gov/articles/PMC3427541/\",\n",
        "        source_human=\"Typical TOJ ~20–80 ms: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0264831 ; recent ranges: https://www.nature.com/articles/s41598-024-84082-z\"),\n",
        "\n",
        "    dict(class_id=\"temporal_causality\", aspect_id=\"sj_threshold_visual\", aspect_label=\"Simultaneity JND (visual)\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.0, Hmin=0.02, Hmax=0.05,\n",
        "        source_universal=\"SJs vs TOJs review: https://www.sciencedirect.com/science/article/abs/pii/S0306452215004431\",\n",
        "        source_human=\"Visual SJ thresholds tens of ms: https://pmc.ncbi.nlm.nih.gov/articles/PMC3427541/\"),\n",
        "\n",
        "    dict(class_id=\"temporal_causality\", aspect_id=\"michotte_launching_gap\", aspect_label=\"Causal launching gap\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.0, Hmin=0.01, Hmax=0.15,\n",
        "        source_universal=\"Perceptual causality (space–time): https://pmc.ncbi.nlm.nih.gov/articles/PMC2868299/\",\n",
        "        source_human=\"Michotte-style temporal contiguity window: https://biomotionlab.ca/Text/PP_Guski.pdf ; overview: https://pmc.ncbi.nlm.nih.gov/articles/PMC9617506/\"),\n",
        "\n",
        "    dict(class_id=\"temporal_causality\", aspect_id=\"intentional_binding_mag\", aspect_label=\"Intentional binding (magnitude)\", unit=\"seconds\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=0.40, Hmin=0.03, Hmax=0.15,\n",
        "        source_universal=\"IB as 0–1 s-scale bias proxy: https://link.springer.com/article/10.3758/s13414-017-1292-y\",\n",
        "        source_human=\"IB ~50–150 ms typical; stability across modalities: https://www.sciencedirect.com/science/article/abs/pii/S1053810024000941 ; see debate: https://www.biorxiv.org/content/10.1101/2023.02.06.526214v3.full-text\"),\n",
        "\n",
        "    dict(class_id=\"temporal_causality\", aspect_id=\"event_segmentation_timescale\", aspect_label=\"Event segmentation timescale\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=0.1, Umax=600.0, Hmin=1.0, Hmax=30.0,\n",
        "        source_universal=\"Event Segmentation Theory: https://www.sciencedirect.com/science/article/abs/pii/S1364661307003312\",\n",
        "        source_human=\"Multiple timescales; boundaries seconds–tens of seconds: https://pmc.ncbi.nlm.nih.gov/articles/PMC2263140/ ; overview: https://www.frontiersin.org/articles/10.3389/fnhum.2010.00168/full\"),\n",
        "\n",
        "    dict(class_id=\"temporal_causality\", aspect_id=\"postdictive_causality_window\", aspect_label=\"Postdictive causality window\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.0, Hmin=0.03, Hmax=0.20,\n",
        "        source_universal=\"Spatiotemporal constraints on causal perception: https://www.sciencedirect.com/science/article/abs/pii/S0010028508000108\",\n",
        "        source_human=\"Delays reducing perceived causality tens–hundreds ms: https://pmc.ncbi.nlm.nih.gov/articles/PMC2868299/\"),\n",
        "\n",
        "###### agency_control\n",
        "\n",
        "    dict(class_id=\"agency_control\", aspect_id=\"action_outcome_delay_window\", aspect_label=\"Action→outcome agency window\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=2.0, Hmin=0.005, Hmax=0.30,\n",
        "        source_universal=\"Temporal credit assignment window for agency judgments (review)\",\n",
        "        source_human=\"Agency declines with increasing delay; typical tolerance <~300 ms (Haggard 2017 review): https://wexler.free.fr/library/files/haggard%20(2017)%20sense%20of%20agency%20in%20the%20human%20brain.pdf ; Delay modulates agency (Erdoğan et al. 2024): https://pmc.ncbi.nlm.nih.gov/articles/PMC12181639/\"),\n",
        "\n",
        "    dict(class_id=\"agency_control\", aspect_id=\"visuomotor_delay_tolerance\", aspect_label=\"Visuomotor adaptation delay tolerance\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.5, Hmin=0.005, Hmax=0.30,\n",
        "        source_universal=\"Feedback delay parameterization in human-in-the-loop control\",\n",
        "        source_human=\"Adaptation persists but reduced with ~200 ms visual delay (Honda et al. 2012 PLoS ONE): https://pmc.ncbi.nlm.nih.gov/articles/PMC3364281/ ; Delayed feedback disrupts error-based learning (Kitazawa 2016 review): https://pmc.ncbi.nlm.nih.gov/articles/PMC4808111/\"),\n",
        "\n",
        "    dict(class_id=\"agency_control\", aspect_id=\"speech_auditory_delay_tolerance\", aspect_label=\"Speech auditory-motor delay tolerance\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1.0, Hmin=0.005, Hmax=0.10,\n",
        "        source_universal=\"Auditory feedback timing in vocal control\",\n",
        "        source_human=\"Auditory-motor adaptation eliminated at ≥100 ms delay (Max & Maffett 2015): https://pmc.ncbi.nlm.nih.gov/articles/PMC4363140/\"),\n",
        "\n",
        "    dict(class_id=\"agency_control\", aspect_id=\"somatosensory_attenuation_timing\", aspect_label=\"Somatosensory attenuation timing window\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=0.5, Hmin=0.005, Hmax=0.15,\n",
        "        source_universal=\"Predictive timing for self-touch in internal models\",\n",
        "        source_human=\"Baseline attenuation strongest near 0 ms; shifts with delay training (~100–150 ms) (Kilteni et al. eLife 2019): https://pmc.ncbi.nlm.nih.gov/articles/PMC6860990/ ; Efference copy necessary (iScience 2020): https://pubmed.ncbi.nlm.nih.gov/32058957/ ; Neural recalibration with 100 ms delays (Nat Commun 2024): https://www.nature.com/articles/s42003-024-06188-4\"),\n",
        "\n",
        "    dict(class_id=\"agency_control\", aspect_id=\"assisted_control_agency\", aspect_label=\"Agency under assistance (relative)\", unit=\"fraction\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=1.0, Hmin=0.50, Hmax=1.00,\n",
        "        source_universal=\"Assistance–agency scale 0–1\",\n",
        "        source_human=\"Agency during continuous action increases with performance under assistance (Wen & Haggard 2015): https://pmc.ncbi.nlm.nih.gov/articles/PMC4404253/\"),\n",
        "\n",
        "####### symbolic_abstract\n",
        "\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_id=\"ans_weber_fraction\", aspect_label=\"Numerosity Weber fraction (ANS)\", unit=\"fraction\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=1.0, Hmin=0.08, Hmax=0.25,\n",
        "        source_universal=\"Weber fraction 0–1 range; ANS follows Weber-like scaling (Testolin & Boninsegna 2021): https://link.springer.com/article/10.3758/s13423-020-01801-z\",\n",
        "        source_human=\"Adults typically ~0.1–0.2; developmental trajectory (Halberda & Feigenson 2008): https://panamath.org/papers/HalberdaFeigenson2008DevPsych.pdf ; overview (Halberda & Odic 2014): https://www.halberdalab.net/files/HalberdaOdic2014WeberChapter.pdf\"),\n",
        "\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_id=\"pitch_jnd_fraction\", aspect_label=\"Pitch JND (Δf/f)\", unit=\"fraction\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=0.10, Hmin=0.0015, Hmax=0.005,\n",
        "        source_universal=\"Δf/f expressed 0–1; psychoacoustic overview (Oxenham 2012): https://pubmed.ncbi.nlm.nih.gov/23015422/\",\n",
        "        source_human=\"~0.15% musicians, ~0.5% non-musicians near mid-frequencies (Micheyl et al. 2006): https://audition.ens.fr/P2web/eval2006/DP_Micheyl-2006.pdf ; review (McDermott & Oxenham 2008): https://mcdermottlab.mit.edu/papers/McDermott_Oxenham_2008_pitch_music_CONB_review.pdf\"),\n",
        "\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_id=\"interval_discrimination_cents\", aspect_label=\"Musical interval JND\", unit=\"cents\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=200.0, Hmin=10.0, Hmax=40.0,\n",
        "        source_universal=\"Interval JND cast on 0–200 cents window for comparability (minor third ≈ 300 cents upper bound truncated for page layout)\",\n",
        "        source_human=\"JNDs ~20–40 cents in non-musicians; better in musicians (Zarate et al. 2012): https://pmc.ncbi.nlm.nih.gov/articles/PMC3427364/ ; 20-cent detection near threshold (Schellenberg 2001): https://www.jstor.org/stable/10.1525/mp.2001.19.2.223 ; corroborating review/data: https://pmc.ncbi.nlm.nih.gov/articles/PMC3455123/\"),\n",
        "\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_id=\"phoneme_vot_boundary_bapa\", aspect_label=\"Phoneme VOT boundary (/b/–/p/)\", unit=\"seconds\", scale=\"log\",\n",
        "        Umin=1e-3, Umax=1e-1, Hmin=0.020, Hmax=0.040,\n",
        "        source_universal=\"VOT continuum cast on 1–100 ms for boundary localization comparisons\",\n",
        "        source_human=\"English bilabial /b/–/p/ boundary ~+20–30 ms; cross-linguistic reviews (Abramson 2017): https://pmc.ncbi.nlm.nih.gov/articles/PMC5665574/ ; boundary examples ~25 ms (Hay 2005): https://repositories.lib.utexas.edu/bitstreams/9e8677cb-ed77-4341-b6b0-ec7e21ccfe11/download ; summary chapter: https://www.degruyterbrill.com/document/doi/10.21832/9781847693761-006/html\"),\n",
        "\n",
        "####### mathematical_cognition\n",
        "\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_id=\"subitizing_capacity\", aspect_label=\"Subitizing capacity\", unit=\"items\", scale=\"linear\",\n",
        "        Umin=1, Umax=100, Hmin=1, Hmax=4,\n",
        "        source_universal=\"Enumeration scale reference 1–100 items\",\n",
        "        source_human=\"Adults subitize ~1–4 items (Kaufman et al., 1949; Trick & Pylyshyn, 1994)\"),\n",
        "\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_id=\"enumeration_counting_slope\", aspect_label=\"Enumeration counting slope\", unit=\"ms/item\", scale=\"linear\",\n",
        "        Umin=0, Umax=1000, Hmin=250, Hmax=350,\n",
        "        source_universal=\"Counting rate expressed as ms per item (0–1000 ms limits)\",\n",
        "        source_human=\"Outside subitizing, slopes ~250–350 ms/item (Trick & Pylyshyn, 1994; Mandler & Shebo, 1982)\"),\n",
        "\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_id=\"addition_problem_size_slope\", aspect_label=\"Addition problem-size slope\", unit=\"ms per sum increment\", scale=\"linear\",\n",
        "        Umin=0, Umax=200, Hmin=20, Hmax=70,\n",
        "        source_universal=\"Problem-size effect cast as ms per unit increase in sum\",\n",
        "        source_human=\"Single-digit addition slopes ~20–70 ms/increment (Ashcraft, 1992; Uittenhove et al., 2016)\"),\n",
        "\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_id=\"operation_span_capacity\", aspect_label=\"Operation span capacity (OSPAN)\", unit=\"items\", scale=\"linear\",\n",
        "        Umin=0, Umax=10, Hmin=3, Hmax=7,\n",
        "        source_universal=\"Complex span scale 0–10 items\",\n",
        "        source_human=\"Typical adult OSPAN ~3–7 items (Unsworth et al., 2005)\"),\n",
        "\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_id=\"number_line_error_0_100\", aspect_label=\"Number line error (0–100)\", unit=\"fraction\", scale=\"linear\",\n",
        "        Umin=0.0, Umax=1.0, Hmin=0.02, Hmax=0.06,\n",
        "        source_universal=\"Percent absolute error expressed as fraction 0–1\",\n",
        "        source_human=\"Adult PAE low (~2–6%) on 0–100 tasks (Siegler & Opfer, 2003; Booth & Siegler, 2006)\")\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "refs_df = pd.DataFrame([\n",
        "    # physical_body\n",
        "    dict(class_id=\"physical_body\", aspect_label=\"Vision (wavelength)\", Hmin=380e-9, Hmax=740e-9, Umin=1e-12, Umax=1e4, scale=\"log\", source_human=\"NASA visible. NASA EMS\"),\n",
        "    dict(class_id=\"physical_body\", aspect_label=\"Vision luminance\", Hmin=1e-6, Hmax=1e6, Umin=1e-9, Umax=1e9, scale=\"log\", source_human=\"Light adaptation. OOM luminance\"),\n",
        "    dict(classid=\"physical_body\", aspect_label=\"Hearing (frequency)\", Hmin=20.0, Hmax=20000.0, Umin=1e-1, Umax=1e7, scale=\"log\", source_human=\"NCBI human range. Hearing ranges\"),\n",
        "    dict(class_id=\"physical_body\", aspect_label=\"Hearing level (pressure)\", Hmin=2e-5, Hmax=2e1, Umin=1e-6, Umax=1e7, scale=\"log\", source_human=\"SPL reference. Sound pressure\"),\n",
        "    dict(class_id=\"physical_body\", aspect_label=\"Touch vibration\", Hmin=2.0, Hmax=1000.0, Umin=1e-1, Umax=1e6, scale=\"log\", source_human=\"Bolanowski 1988. Mechanical vibration\"),\n",
        "    dict(class_id=\"physical_body\", aspect_label=\"Touch spatial resolution\", Hmin=2e-3, Hmax=4e-2, Umin=1e-9, Umax=1.0, scale=\"log\", source_human=\"Weinstein 1968. Contact scale\"),\n",
        "    dict(class_id=\"physical_body\", aspect_label=\"Temporal gap detection\", Hmin=2e-3, Hmax=1e-1, Umin=1e-15, Umax=3.1536e9, scale=\"log\", source_human=\"Gap detection review. Time scales\"),\n",
        "\n",
        "    # multisensory\n",
        "    dict(class_id=\"multisensory\", aspect_label=\"Audio–visual TBW (flash/beep)\", Hmin=0.08, Hmax=0.20, Umin=1e-3, Umax=10.0, scale=\"log\", source_human=\"Typical adult TBW ~160 ms for simple AV (Zerr et al. 2019): https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02489/full. TBW construct review (Wallace & Stevenson 2014): https://pmc.ncbi.nlm.nih.gov/articles/PMC4326640/\"),\n",
        "    dict(class_id=\"multisensory\", aspect_label=\"Audio–visual TBW (speech)\", Hmin=0.12, Hmax=0.30, Umin=1e-3, Umax=10.0, scale=\"log\", source_human=\"Speech TBW often wider ~200–250 ms (Hillock-Dunn et al. 2016): https://www.sciencedirect.com/science/article/abs/pii/S0028393216300525. Intersensory synchrony tutorial (Vroomen & Keetels 2010): https://pubmed.ncbi.nlm.nih.gov/20436185/\"),\n",
        "    dict(class_id=\"multisensory\", aspect_label=\"Visual–haptic MLE consistency\", Hmin=0.70, Hmax=1.00, Umin=0.0, Umax=1.0, scale=\"linear\", source_human=\"Near-optimal integration (Ernst & Banks 2002 Nature): https://pubmed.ncbi.nlm.nih.gov/11807554/; Tool-mediated MLE (Takahashi et al. 2017): https://pmc.ncbi.nlm.nih.gov/articles/PMC5380699/\"),\n",
        "    dict(class_id=\"multisensory\", aspect_label=\"Ventriloquism spatial bias\", Hmin=1.0, Hmax=10.0, Umin=0.0, Umax=45.0, scale=\"linear\", source_human=\"Near-optimal AV spatial integration; bias scales with disparity (Alais & Burr 2004): https://pubmed.ncbi.nlm.nih.gov/14761661/. Ventriloquism paradigms/review (Bruns 2019): https://www.frontiersin.org/articles/10.3389/fnint.2019.00051/full\"),\n",
        "    dict(class_id=\"multisensory\", aspect_label=\"Sound-induced flash TBW\", Hmin=0.05, Hmax=0.25, Umin=1e-3, Umax=1.0, scale=\"log\", source_human=\"SIFI temporal window tens–hundreds of ms (Hirst et al. 2020 review): https://www.sciencedirect.com/science/article/pii/S0149763420305637. Task-dependent AV TBW framework (Stevenson & Wallace 2013): https://pmc.ncbi.nlm.nih.gov/articles/PMC3711231/\"),\n",
        "    dict(class_id=\"multisensory\", aspect_label=\"McGurk susceptibility\", Hmin=0.10, Hmax=0.80, Umin=0.0, Umax=1.0, scale=\"linear\", source_human=\"Large variability across adults/stimuli (Mallick et al. 2015): https://pmc.ncbi.nlm.nih.gov/articles/PMC4580505/; ranges 0–100% observed (Magnotti et al. 2024): https://www.sciencedirect.com/science/article/pii/S0010945220303749\"),\n",
        "\n",
        "    # temporal_causality\n",
        "    dict(class_id=\"temporal_causality\", aspect_label=\"Temporal order JND\", Hmin=0.02, Hmax=0.08, Umin=1e-3, Umax=1.0, scale=\"log\", source_human=\"Typical TOJ ~20–80 ms: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0264831 ; recent ranges: https://www.nature.com/articles/s41598-024-84082-z. TOJ paradigms overview: https://pmc.ncbi.nlm.nih.gov/articles/PMC3427541/\"),\n",
        "    dict(class_id=\"temporal_causality\", aspect_label=\"Simultaneity JND (visual)\", Hmin=0.02, Hmax=0.05, Umin=1e-3, Umax=1.0, scale=\"log\", source_human=\"Visual SJ thresholds tens of ms: https://pmc.ncbi.nlm.nih.gov/articles/PMC3427541/. SJs vs TOJs review: https://www.sciencedirect.com/science/article/abs/pii/S0306452215004431\"),\n",
        "    dict(class_id=\"temporal_causality\", aspect_label=\"Causal launching gap\", Hmin=0.01, Hmax=0.15, Umin=1e-3, Umax=1.0, scale=\"log\", source_human=\"Michotte-style temporal contiguity window: https://biomotionlab.ca/Text/PP_Guski.pdf ; overview: https://pmc.ncbi.nlm.nih.gov/articles/PMC9617506/. Perceptual causality (space–time): https://pmc.ncbi.nlm.nih.gov/articles/PMC2868299/\"),\n",
        "    dict(class_id=\"temporal_causality\", aspect_label=\"Intentional binding (magnitude)\", Hmin=0.03, Hmax=0.15, Umin=0.0, Umax=0.40, scale=\"linear\", source_human=\"IB ~50–150 ms typical; stability across modalities: https://www.sciencedirect.com/science/article/abs/pii/S1053810024000941 ; see debate: https://www.biorxiv.org/content/10.1101/2023.02.06.526214v3.full-text\"),\n",
        "    dict(class_id=\"temporal_causality\", aspect_label=\"Event segmentation timescale\", Hmin=1.0, Hmax=30.0, Umin=0.1, Umax=600.0, scale=\"log\", source_human=\"Multiple timescales; boundaries seconds–tens of seconds: https://pmc.ncbi.nlm.nih.gov/articles/PMC2263140/ ; overview: https://www.frontiersin.org/articles/10.3389/fnhum.2010.00168/full. Event Segmentation Theory: https://www.sciencedirect.com/science/article/abs/pii/S1364661307003312\"),\n",
        "    dict(class_id=\"temporal_causality\", aspect_label=\"Postdictive causality window\", Hmin=0.03, Hmax=0.20, Umin=1e-3, Umax=1.0, scale=\"log\", source_human=\"Delays reducing perceived causality tens–hundreds ms: https://pmc.ncbi.nlm.nih.gov/articles/PMC2868299/. Spatiotemporal constraints on causal perception: https://www.sciencedirect.com/science/article/abs/pii/S0010028508000108\"),\n",
        "\n",
        "    # agency_control\n",
        "    dict(class_id=\"agency_control\", aspect_label=\"Action→outcome agency window\", Hmin=0.005, Hmax=0.30, Umin=1e-3, Umax=2.0, scale=\"log\", source_human=\"Agency declines with increasing delay; typical tolerance <~300 ms (Haggard 2017 review): https://wexler.free.fr/library/files/haggard%20(2017)%20sense%20of%20agency%20in%20the%20human%20brain.pdf ; Delay modulates agency (Erdoğan et al. 2024): https://pmc.ncbi.nlm.nih.gov/articles/PMC12181639/\"),\n",
        "    dict(class_id=\"agency_control\", aspect_label=\"Visuomotor adaptation delay tolerance\", Hmin=0.005, Hmax=0.30, Umin=1e-3, Umax=1.5, scale=\"log\", source_human=\"Adaptation persists but reduced with ~200 ms visual delay (Honda et al. 2012 PLoS ONE): https://pmc.ncbi.nlm.nih.gov/articles/PMC3364281/ ; Delayed feedback disrupts error-based learning (Kitazawa 2016 review): https://pmc.ncbi.nlm.nih.gov/articles/PMC4808111/\"),\n",
        "    dict(class_id=\"agency_control\", aspect_label=\"Speech auditory-motor delay tolerance\", Hmin=0.005, Hmax=0.10, Umin=1e-3, Umax=1.0, scale=\"log\", source_human=\"Auditory-motor adaptation eliminated at ≥100 ms delay (Max & Maffett 2015): https://pmc.ncbi.nlm.nih.gov/articles/PMC4363140/\"),\n",
        "    dict(class_id=\"agency_control\", aspect_label=\"Somatosensory attenuation timing window\", Hmin=0.005, Hmax=0.15, Umin=1e-3, Umax=0.5, scale=\"log\", source_human=\"Baseline attenuation strongest near 0 ms; shifts with delay training (~100–150 ms) (Kilteni et al. eLife 2019): https://pmc.ncbi.nlm.nih.gov/articles/PMC6860990/ ; Efference copy necessary (iScience 2020): https://pubmed.ncbi.nlm.nih.gov/32058957/ ; Neural recalibration with 100 ms delays (Nat Commun 2024): https://www.nature.com/articles/s42003-024-06188-4\"),\n",
        "    dict(class_id=\"agency_control\", aspect_label=\"Agency under assistance (relative)\", Hmin=0.50, Hmax=1.00, Umin=0.0, Umax=1.0, scale=\"linear\", source_human=\"Agency during continuous action increases with performance under assistance (Wen & Haggard 2015): https://pmc.ncbi.nlm.nih.gov/articles/PMC4404253/\"),\n",
        "\n",
        "    # symbolic_abstract\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_label=\"Numerosity Weber fraction (ANS)\", Hmin=0.08, Hmax=0.25, Umin=0.0, Umax=1.0, scale=\"linear\", source_human=\"Adults typically ~0.1–0.2; developmental trajectory (Halberda & Feigenson 2008): https://panamath.org/papers/HalberdaFeigenson2008DevPsych.pdf ; overview (Halberda & Odic 2014): https://www.halberdalab.net/files/HalberdaOdic2014WeberChapter.pdf\"),\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_label=\"Pitch JND (Δf/f)\", Hmin=0.0015, Hmax=0.005, Umin=0.0, Umax=0.10, scale=\"linear\", source_human=\"~0.15% musicians, ~0.5% non-musicians near mid-frequencies (Micheyl et al. 2006): https://audition.ens.fr/P2web/eval2006/DP_Micheyl-2006.pdf ; review (McDermott & Oxenham 2008): https://mcdermottlab.mit.edu/papers/McDermott_Oxenham_2008_pitch_music_CONB_review.pdf\"),\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_label=\"Musical interval JND\", Hmin=10.0, Hmax=40.0, Umin=0.0, Umax=200.0, scale=\"linear\", source_human=\"JNDs ~20–40 cents in non-musicians; better in musicians (Zarate et al. 2012): https://pmc.ncbi.nlm.nih.gov/articles/PMC3427364/ ; 20-cent detection near threshold (Schellenberg 2001): https://www.jstor.org/stable/10.1525/mp.2001.19.2.223 ; corroborating review/data: https://pmc.ncbi.nlm.nih.gov/articles/PMC3455123/\"),\n",
        "    dict(class_id=\"symbolic_abstract\", aspect_label=\"Phoneme VOT boundary (/b/–/p/)\", Hmin=0.020, Hmax=0.040, Umin=1e-3, Umax=1e-1, scale=\"log\", source_human=\"English bilabial /b/–/p/ boundary ~+20–30 ms; cross-linguistic reviews (Abramson 2017): https://pmc.ncbi.nlm.nih.gov/articles/PMC5665574/ ; boundary examples ~25 ms (Hay 2005): https://repositories.lib.utexas.edu/bitstreams/9e8677cb-ed77-4341-b6b0-ec7e21ccfe11/download\"),\n",
        "\n",
        "    # mathematical_cognition\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_label=\"Subitizing capacity\", Hmin=1, Hmax=4, Umin=1, Umax=100, scale=\"linear\", source_human=\"Adults subitize ~1–4 items (Kaufman et al., 1949; Trick & Pylyshyn, 1994)\"),\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_label=\"Enumeration counting slope\", Hmin=250, Hmax=350, Umin=0, Umax=1000, scale=\"linear\", source_human=\"Outside subitizing, slopes ~250–350 ms/item (Trick & Pylyshyn, 1994; Mandler & Shebo, 1982)\"),\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_label=\"Addition problem-size slope\", Hmin=20, Hmax=70, Umin=0, Umax=200, scale=\"linear\", source_human=\"Single-digit addition slopes ~20–70 ms/increment (Ashcraft, 1992; Uittenhove et al., 2016)\"),\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_label=\"Operation span capacity (OSPAN)\", Hmin=3, Hmax=7, Umin=0, Umax=10, scale=\"linear\", source_human=\"Typical adult OSPAN ~3–7 items (Unsworth et al., 2005)\"),\n",
        "    dict(class_id=\"mathematical_cognition\", aspect_label=\"Number line error (0–100)\", Hmin=0.02, Hmax=0.06, Umin=0.0, Umax=1.0, scale=\"linear\", source_human=\"Adult PAE low (~2–6%) on 0–100 tasks (Siegler & Opfer, 2003; Booth & Siegler, 2006)\"),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "x1b1DnbgOMjY"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import textwrap\n",
        "\n",
        "# --- Robust Font Selection ---\n",
        "# Find the best available sans-serif font on the system for a professional look.\n",
        "# This avoids fragile web downloads and works reliably in Colab.\n",
        "font_family = 'Roboto'\n",
        "try:\n",
        "    # Check if Roboto is available\n",
        "    fm.findfont(font_family, fallback_to_default=False)\n",
        "except ValueError:\n",
        "    # If not, fall back to a common, high-quality sans-serif font\n",
        "    print(\"Roboto not found, falling back to DejaVu Sans.\")\n",
        "    font_family = 'DejaVu Sans'\n",
        "\n",
        "\n",
        "# --- Design System (Colors & Fonts) ---\n",
        "# A consistent color palette and typography for a cohesive design\n",
        "BG_COLOR = \"#f5f5f5\"\n",
        "TEXT_COLOR = \"#222222\"\n",
        "SUBTLE_TEXT_COLOR = \"#666666\"\n",
        "HIGHLIGHT_COLOR = \"#007acc\"\n",
        "\n",
        "# Assign a unique, modern color to each perception class\n",
        "CLASS_COLORS = {\n",
        "    'physical_body': '#1f77b4',\n",
        "    'multisensory': '#ff7f0e',\n",
        "    'temporal_causality': '#2ca02c',\n",
        "    'agency_control': '#d62728',\n",
        "    'symbolic_abstract': '#9467bd',\n",
        "    'mathematical_cognition': '#8c564b'\n",
        "}\n",
        "\n",
        "# --- Global Matplotlib Styling ---\n",
        "# Apply our design system to all future plots\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': BG_COLOR,\n",
        "    'axes.facecolor': BG_COLOR,\n",
        "    'axes.edgecolor': TEXT_COLOR,\n",
        "    'axes.labelcolor': TEXT_COLOR,\n",
        "    'axes.titlecolor': TEXT_COLOR,\n",
        "    'xtick.color': TEXT_COLOR,\n",
        "    'ytick.color': TEXT_COLOR,\n",
        "    'text.color': TEXT_COLOR,\n",
        "    'font.family': font_family,\n",
        "    'font.weight': 'normal',\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelweight': 'normal',\n",
        "    'hatch.linewidth': 0.5\n",
        "})\n",
        "\n",
        "print(f\"Using font: '{font_family}' for the report.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y_P0U-gOMoe",
        "outputId": "3392ae2c-d124-4099-91e7-1ba91c33c32e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roboto not found, falling back to DejaVu Sans.\n",
            "Using font: 'DejaVu Sans' for the report.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _valid_row(r):\n",
        "    if r[\"Umin\"]>=r[\"Umax\"] or r[\"Hmin\"]>=r[\"Hmax\"]: return False\n",
        "    if not (r[\"Umin\"]<=r[\"Hmin\"]<=r[\"Hmax\"]<=r[\"Umax\"]): return False\n",
        "    if r[\"scale\"]==\"log\" and (r[\"Umin\"]<=0 or r[\"Umax\"]<=0 or r[\"Hmin\"]<=0 or r[\"Hmax\"]<=0): return False\n",
        "    return True\n",
        "\n",
        "def _frac(x, umin, umax, scale):\n",
        "    if scale==\"log\": return (math.log10(x)-math.log10(umin))/(math.log10(umax)-math.log10(umin))\n",
        "    return (x-umin)/(umax-umin)\n",
        "\n",
        "def _span_fraction(hmin, hmax, umin, umax, scale):\n",
        "    if scale==\"log\": return (math.log10(hmax)-math.log10(hmin))/(math.log10(umax)-math.log10(umin))\n",
        "    return (hmax-hmin)/(umax-umin)\n",
        "\n",
        "def _u_weight(umin, umax, scale):\n",
        "    if scale==\"log\": return math.log10(umax)-math.log10(umin)\n",
        "    return umax-umin\n",
        "\n",
        "def compute_metrics(refs):\n",
        "    df = refs.copy()\n",
        "    df = df[df.apply(_valid_row, axis=1)].reset_index(drop=True)\n",
        "    df[\"fmin\"] = [_frac(hmin, umin, umax, s) for hmin,umin,umax,s in zip(df.Hmin,df.Umin,df.Umax,df.scale)]\n",
        "    df[\"fmax\"] = [_frac(hmax, umin, umax, s) for hmax,umin,umax,s in zip(df.Hmax,df.Umin,df.Umax,df.scale)]\n",
        "    df[\"coverage_fraction\"] = [_span_fraction(hmin,hmax,umin,umax,s) for hmin,hmax,umin,umax,s in zip(df.Hmin,df.Hmax,df.Umin,df.Umax,df.scale)]\n",
        "    df[\"U_weight\"] = [_u_weight(umin,umax,s) for umin,umax,s in zip(df.Umin,df.Umax,df.scale)]\n",
        "    return df\n",
        "\n",
        "def class_aggregates(df):\n",
        "    g = df.groupby(\"class_id\")\n",
        "    agg = g.apply(lambda x: pd.Series(dict(\n",
        "        n_aspects=len(x),\n",
        "        coverage_mean_unweighted=float(x[\"coverage_fraction\"].mean()),\n",
        "        coverage_mean_weighted=float((x[\"coverage_fraction\"]*x[\"U_weight\"]).sum()/x[\"U_weight\"].sum()),\n",
        "        coverage_median=float(x[\"coverage_fraction\"].median()),\n",
        "        coverage_min=float(x[\"coverage_fraction\"].min()),\n",
        "        coverage_max=float(x[\"coverage_fraction\"].max())\n",
        "    ))).reset_index()\n",
        "    return agg"
      ],
      "metadata": {
        "id": "PXE472f3OMtb"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, math, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: STYLE DEFAULTS & CORE DATA FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Style Defaults ---\n",
        "BG_COLOR          = globals().get(\"BG_COLOR\", \"white\")\n",
        "TEXT_COLOR        = globals().get(\"TEXT_COLOR\", \"#111\")\n",
        "SUBTLE_TEXT_COLOR = globals().get(\"SUBTLE_TEXT_COLOR\", \"#666\")\n",
        "HIGHLIGHT_COLOR   = globals().get(\"HIGHLIGHT_COLOR\", \"#2f6ebb\")\n",
        "CLASS_COLORS      = globals().get(\"CLASS_COLORS\", {})\n",
        "THEME_BLUE        = \"#2f6ebb\"\n",
        "\n",
        "# --- Core Data Processing (Used by all report generation) ---\n",
        "def compute_metrics(refs):\n",
        "    df = refs.copy()\n",
        "    df = df[df.apply(lambda r: \"Hmin\" in r and r[\"Hmin\"] < r[\"Hmax\"] and \"Umin\" in r and r[\"Umin\"] < r[\"Umax\"], axis=1)].copy()\n",
        "    def span_fraction(r):\n",
        "        if r[\"scale\"] == \"log\":\n",
        "            if any(x <= 0 for x in [r[\"Umin\"], r[\"Umax\"], r[\"Hmin\"], r[\"Hmax\"]]): return 0.0\n",
        "            return (math.log10(r[\"Hmax\"]) - math.log10(r[\"Hmin\"])) / (math.log10(r[\"Umax\"]) - math.log10(r[\"Umin\"]))\n",
        "        return (r[\"Hmax\"] - r[\"Hmin\"]) / (r[\"Umax\"] - r[\"Umin\"])\n",
        "    def u_span(r):\n",
        "        if r[\"scale\"] == \"log\":\n",
        "            if r[\"Umin\"] <= 0 or r[\"Umax\"] <= 0: return 0.0\n",
        "            return math.log10(r[\"Umax\"]) - math.log10(r[\"Umin\"])\n",
        "        return r[\"Umax\"] - r[\"Umin\"]\n",
        "    df[\"coverage_fraction\"] = df.apply(span_fraction, axis=1)\n",
        "    df[\"U_weight\"] = df.apply(u_span, axis=1)\n",
        "    tot = df[\"U_weight\"].sum()\n",
        "    df[\"U_share\"] = df[\"U_weight\"] / tot if tot > 0 else 0.0\n",
        "    return df\n",
        "\n",
        "def class_aggregates(df):\n",
        "    base = df.groupby(\"class_id\")[\"coverage_fraction\"].agg(\n",
        "        coverage_mean=\"mean\", median=\"median\", min=\"min\", max=\"max\", n_aspects=\"count\"\n",
        "    ).reset_index()\n",
        "    base[\"range\"] = base[\"max\"] - base[\"min\"]\n",
        "    wmean = (df.groupby(\"class_id\")\n",
        "             .apply(lambda x: np.average(x[\"coverage_fraction\"], weights=x[\"U_weight\"]))\n",
        "             .reset_index(name=\"coverage_mean_weighted\"))\n",
        "    return base.merge(wmean, on=\"class_id\", how=\"left\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: NEW - DEDICATED HELPERS FOR THE DETAILED SUMMARY PAGE\n",
        "# These are renamed to prevent conflicts with the other report script.\n",
        "# ==============================================================================\n",
        "\n",
        "def _detail_wrap_label(s, width=14, max_lines=2):\n",
        "    s = re.sub(r\"[_\\-]+\", \" \", str(s)).strip()\n",
        "    words, lines, cur = s.split(), [], \"\"\n",
        "    for w in words:\n",
        "        add = (w if not cur else \" \" + w)\n",
        "        if len(cur + add) <= width: cur += add\n",
        "        else:\n",
        "            lines.append(cur.strip()); cur = w\n",
        "            if len(lines) >= max_lines - 1:\n",
        "                cur = (cur + \" \" + \" \".join(words[words.index(w)+1:])).strip()\n",
        "                break\n",
        "    if cur: lines.append(cur.strip())\n",
        "    return \"\\n\".join(lines[:max_lines])\n",
        "\n",
        "def _detail_class_order_by_pc1(agg):\n",
        "    X = agg[[\"coverage_mean\",\"median\",\"max\",\"range\",\"n_aspects\"]].astype(float).to_numpy()\n",
        "    X = (X - X.mean(0)) / np.where(X.std(0)==0, 1, X.std(0))\n",
        "    u, s, vt = np.linalg.svd(np.nan_to_num(X), full_matrices=False)\n",
        "    pc1 = (X @ vt.T)[:,0]\n",
        "    return agg[\"class_id\"].to_numpy()[np.argsort(pc1)]\n",
        "\n",
        "def _detail_base_positions(agg):\n",
        "    classes = _detail_class_order_by_pc1(agg)\n",
        "    cx = np.linspace(0.06, 0.94, len(classes))\n",
        "    return classes, dict(zip(classes, cx)), (cx[1]-cx[0] if len(cx)>1 else 0.2)\n",
        "\n",
        "def _detail_shrink_center_h(ax, frac=0.10):\n",
        "    p = ax.get_position(); new_w = p.width * (1 - frac); new_x = p.x0 + (p.width - new_w) / 2\n",
        "    ax.set_position([new_x, p.y0, new_w, p.height])\n",
        "\n",
        "def _detail_continuous_intensity(df, agg, width=4200, sigma_mul=0.22, jitter_mul=0.18, gamma=0.8, eq_by_class=True, seed=42):\n",
        "    classes, pos, spacing = _detail_base_positions(agg)\n",
        "    x = np.linspace(0, 1, width); I = np.zeros_like(x)\n",
        "    rng = np.random.default_rng(seed); sigma = max(1e-3, spacing * sigma_mul)\n",
        "    if eq_by_class:\n",
        "        cls_w = df.groupby(\"class_id\")[\"U_weight\"].sum().replace(0, 1.0)\n",
        "        eq = (cls_w.mean() / cls_w).reindex(df[\"class_id\"]).to_numpy()\n",
        "    else:\n",
        "        eq = np.ones(len(df))\n",
        "    for i, r in df.reset_index(drop=True).iterrows():\n",
        "        cov = float(np.clip(r[\"coverage_fraction\"], 0, 1))\n",
        "        cx  = pos.get(r[\"class_id\"], 0.5)\n",
        "        xi  = np.clip(cx + rng.normal(0, spacing*jitter_mul), 0, 1)\n",
        "        w   = float(r.get(\"U_weight\", 1.0)) * eq[i]\n",
        "        I  += w * cov * np.exp(-0.5*((x - xi)/sigma)**2)\n",
        "    if I.max() > 0: I /= I.max()\n",
        "    I = I**gamma\n",
        "    return I, classes, pos\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: PLOTTING FUNCTIONS FOR THE DETAILED REPORT\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_summary_page_v2(df, agg, *,\n",
        "                         height_scales=(0.30, 0.34, 0.36),\n",
        "                         top_shrink=0.10, mid_shrink=0.10,\n",
        "                         field_width=4200, field_sigma=0.22, field_jitter=0.18, field_gamma=0.8,\n",
        "                         labels_target=10, eq_by_class=True):\n",
        "    fig = plt.figure(figsize=(8.5, 11), facecolor=BG_COLOR)\n",
        "    fig.text(0.095, 0.965, \"Human perception — coverage overview\", fontsize=18, weight=\"bold\", color=TEXT_COLOR)\n",
        "    fig.text(0.095, 0.94,  \"\", fontsize=10, color=SUBTLE_TEXT_COLOR)\n",
        "    gs = fig.add_gridspec(3, 1, height_ratios=list(height_scales),\n",
        "                          hspace=0.44, top=0.92, bottom=0.09, left=0.12, right=0.88)\n",
        "    ax_top = fig.add_subplot(gs[0, 0]); ax_mid = fig.add_subplot(gs[1, 0]); ax_bot = fig.add_subplot(gs[2, 0])\n",
        "\n",
        "    # --- Top Plot: Heatmap ---\n",
        "    heat = agg.set_index(\"class_id\")[[\"coverage_mean\",\"median\",\"max\",\"range\",\"n_aspects\"]].astype(float)\n",
        "    normed = heat.copy()\n",
        "    for col in normed.columns:\n",
        "        vmin, vmax = float(normed[col].min()), float(normed[col].max())\n",
        "        normed[col] = 0.5 if abs(vmax - vmin) < 1e-12 else (normed[col] - vmin) / (vmax - vmin)\n",
        "    annot_df = heat.round(2).astype(str)\n",
        "    sns.heatmap(normed, ax=ax_top, annot=annot_df, fmt=\"\", cmap=\"Blues\",\n",
        "                linewidths=.5, linecolor=(0,0,0,0.15), cbar=False, annot_kws={\"size\": 7})\n",
        "    ax_top.set_title(\"Metrics (per-column scaled)\", fontsize=11, pad=8, color=THEME_BLUE)\n",
        "    ax_top.set_ylabel(\"\")\n",
        "    ax_top.tick_params(axis=\"y\", rotation=0, labelsize=8); ax_top.tick_params(axis=\"x\", labelsize=8)\n",
        "    ax_top.set_yticklabels([_detail_wrap_label(t.get_text(), 14, 2) for t in ax_top.get_yticklabels()]) # <-- UPDATED\n",
        "    ax_top.set_xticklabels([_detail_wrap_label(t.get_text(), 10, 2) for t in ax_top.get_xticklabels()]) # <-- UPDATED\n",
        "    _detail_shrink_center_h(ax_top, top_shrink) # <-- UPDATED\n",
        "\n",
        "    # --- Middle Plot: Violin Plots ---\n",
        "    order = agg.sort_values(\"coverage_mean\", ascending=False)[\"class_id\"]\n",
        "    sns.violinplot(ax=ax_mid, y=\"class_id\", x=\"coverage_fraction\", data=df, order=order,\n",
        "                   hue=\"class_id\", palette=CLASS_COLORS if CLASS_COLORS else None,\n",
        "                   inner=\"quartile\", cut=0, legend=False, orient=\"h\", linewidth=0.8)\n",
        "    ax_mid.set_title(\"Distributions (per aspect)\", fontsize=11, pad=8, color=THEME_BLUE)\n",
        "    ax_mid.set_xlabel(\"coverage fraction\", fontsize=9, color=SUBTLE_TEXT_COLOR); ax_mid.set_ylabel(\"\")\n",
        "    ax_mid.tick_params(axis=\"y\", labelsize=8)\n",
        "    ax_mid.spines[[\"right\",\"top\"]].set_visible(False); ax_mid.xaxis.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    _detail_shrink_center_h(ax_mid, mid_shrink) # <-- UPDATED\n",
        "\n",
        "    # --- Bottom Plot: Continuous Field ---\n",
        "    # I, classes, pos = _detail_continuous_intensity(df, agg, width=field_width, sigma_mul=field_sigma, # <-- UPDATED\n",
        "    #                                              jitter_mul=field_jitter, gamma=field_gamma, eq_by_class=eq_by_class)\n",
        "    # y = np.linspace(0, 1, 220)\n",
        "    # vtex = 1 - 0.10 * np.cos(2 * np.pi * y) - 0.05 * np.cos(6 * np.pi * y)\n",
        "    # img = np.clip(np.outer(vtex, I), 0, 1)\n",
        "    # ax_bot.imshow(img, cmap=\"Blues\", origin=\"lower\", aspect=\"auto\", extent=(0,1,0,1))\n",
        "    # ax_bot.set_title(\"Continuous coverage field (equalized by class)\" if eq_by_class else \"Continuous coverage field\",\n",
        "    #                  fontsize=11, pad=8, color=THEME_BLUE)\n",
        "    # ax_bot.axis(\"off\")\n",
        "    # for c in classes:\n",
        "    #     ax_bot.plot([pos[c], pos[c]], [0.14, 0.88], color=(0,0,0,0.22), lw=0.55)\n",
        "    # step = max(1, len(classes)//max(1, labels_target)); show = classes[::step][:labels_target]\n",
        "    # for c in show:\n",
        "    #     ax_bot.text(pos[c], 0.08, _detail_wrap_label(c, 14, 2), ha=\"center\", va=\"top\", fontsize=8, color=TEXT_COLOR) # <-- UPDATED\n",
        "\n",
        "    return fig\n",
        "\n",
        "def plot_class_page_letter_v2(df_class, agg_row, page_num):\n",
        "    def _fraction_bounds(r):\n",
        "        try:\n",
        "            if r[\"scale\"] == \"log\":\n",
        "                if r[\"Umin\"]<=0 or r[\"Umax\"]<=0 or r[\"Hmin\"]<=0 or r[\"Hmax\"]<=0: return np.nan, np.nan\n",
        "                du = np.log10(r[\"Umax\"]) - np.log10(r[\"Umin\"]);\n",
        "                if du<=0: return np.nan, np.nan\n",
        "                fmin = (np.log10(r[\"Hmin\"]) - np.log10(r[\"Umin\"])) / du\n",
        "                fmax = (np.log10(r[\"Hmax\"]) - np.log10(r[\"Umin\"])) / du\n",
        "            else:\n",
        "                du = (r[\"Umax\"] - r[\"Umin\"]);\n",
        "                if du<=0: return np.nan, np.nan\n",
        "                fmin = (r[\"Hmin\"] - r[\"Umin\"]) / du\n",
        "                fmax = (r[\"Hmax\"] - r[\"Umin\"]) / du\n",
        "            return np.clip(fmin,0,1), np.clip(fmax,0,1)\n",
        "        except Exception: return np.nan, np.nan\n",
        "\n",
        "    bars = df_class.copy(); bars[[\"fmin\",\"fmax\"]] = bars.apply(_fraction_bounds, axis=1, result_type=\"expand\")\n",
        "    bars = bars.dropna(subset=[\"fmin\",\"fmax\"]); bars[\"span\"] = (bars[\"fmax\"] - bars[\"fmin\"]).clip(lower=0)\n",
        "    order = bars.sort_values(\"coverage_fraction\", ascending=True).reset_index(drop=True); n = len(order)\n",
        "    fig = plt.figure(figsize=(8.5, 11), facecolor=BG_COLOR)\n",
        "    fig.text(0.1, 0.95, \"Human Perceptual Coverage Report\", fontsize=14, weight='bold', color=TEXT_COLOR)\n",
        "    fig.text(0.9, 0.95, f\"Domain: {agg_row['class_id']}\", ha='right', fontsize=12, color=SUBTLE_TEXT_COLOR)\n",
        "    fig.patches.extend([plt.Rectangle((0.1, 0.93), 0.8, 0.002, fc=SUBTLE_TEXT_COLOR, transform=fig.transFigure, alpha=0.5)])\n",
        "    ax_top = fig.add_axes([0.3, 0.55, 0.55, 0.35])\n",
        "    class_color = CLASS_COLORS.get(agg_row['class_id'], '#6ea8dc')\n",
        "    ax_top.barh(range(n), order[\"span\"], left=order[\"fmin\"], height=0.6, color=class_color, alpha=0.75)\n",
        "    ax_top.set_title(f\"Aspect windows within universal span — {agg_row['class_id']}\", loc='left', fontsize=12, pad=12)\n",
        "    ax_top.set_xlabel(\"fraction of universal span (declared scale)\", fontsize=9); ax_top.set_xlim(0,1)\n",
        "    ax_top.set_ylim(-0.5, max(-0.5, n-0.5)); ax_top.set_yticks(range(n)); ax_top.set_yticklabels([])\n",
        "    ax_top.xaxis.grid(True, linestyle='--', color=SUBTLE_TEXT_COLOR, alpha=0.5)\n",
        "    ax_top.spines[['right','top','left']].set_visible(False); ax_top.tick_params(axis='y', length=0)\n",
        "    axpos = ax_top.get_position()\n",
        "    for i, r in order.iterrows():\n",
        "        fig.text(axpos.x0 - 0.01, axpos.y0 + (i + 0.5) / n * axpos.height, str(r[\"aspect_label\"]), ha=\"right\", va=\"center\", fontsize=8, color=TEXT_COLOR)\n",
        "        ax_top.text(1.01, i, f\"{r['coverage_fraction']*100:.1f}%\", ha=\"left\", va=\"center\", fontsize=8, color=TEXT_COLOR)\n",
        "        src = r.get(\"source_human\", \"\");\n",
        "        if isinstance(src, str) and src.startswith((\"http://\",\"https://\")):\n",
        "            ax_top.text(1.10, i, \"source\", ha=\"left\", va=\"center\", fontsize=8, color=HIGHLIGHT_COLOR, url=src)\n",
        "    gs_bottom = fig.add_gridspec(1, 2, top=0.45, bottom=0.1, left=0.1, right=0.9, wspace=0.4)\n",
        "    ax_left = fig.add_subplot(gs_bottom[0,0]); ax_right = fig.add_subplot(gs_bottom[0,1], projection=\"polar\")\n",
        "    ax_left.axis(\"off\"); ax_left.set_title(\"Domain summary\", loc='left', fontsize=12, pad=8, color=THEME_BLUE)\n",
        "    w = bars[\"U_weight\"].to_numpy()\n",
        "    wmean = (np.average(bars[\"coverage_fraction\"], weights=w) if np.isfinite(w).all() and w.sum()>0 else float(bars[\"coverage_fraction\"].mean()))\n",
        "    stats = [(\"Aspects\", f\"{int(len(bars))}\"),(\"Weighted mean\", f\"{wmean*100:.1f}%\"),(\"Median\", f\"{bars['coverage_fraction'].median()*100:.1f}%\"),(\"Range\", f\"{bars['coverage_fraction'].min()*100:.1f}% – {bars['coverage_fraction'].max()*100:.1f}%\")]\n",
        "    for i, (k, v) in enumerate(stats):\n",
        "        y = 0.82 - i*0.2; ax_left.text(0, y, k, ha='left', va='top', fontsize=10, weight='bold', color=TEXT_COLOR)\n",
        "        ax_left.text(0, y-0.08, v, ha='left', va='top', fontsize=14, color=TEXT_COLOR)\n",
        "    ax_left.text(0, 0.02, \"Coverage = span(H)/span(U). Bars normalized 0–1.\", fontsize=8, color=SUBTLE_TEXT_COLOR)\n",
        "    def _radar_axes(n): ang = np.linspace(0, 2*np.pi, n, endpoint=False); return np.r_[ang, ang[:1]]\n",
        "    def plot_radar(ax, labels, values, color):\n",
        "        n=len(labels); ang=_radar_axes(n); vals=np.r_[values,values[:1]]; ax.set_theta_offset(np.pi/2)\n",
        "        ax.set_theta_direction(-1); ax.set_ylim(0,1); ax.plot(ang,vals,lw=2,color=color); ax.fill(ang,vals,alpha=0.25,color=color)\n",
        "        ax.set_xticks(_radar_axes(n)[:-1]); ax.set_xticklabels(labels,fontsize=7); ax.set_yticks([0.25,0.5,0.75])\n",
        "        ax.set_yticklabels([\"0.25\",\"0.5\",\"0.75\"],fontsize=7)\n",
        "    plot_radar(ax_right, order[\"aspect_label\"].tolist(), order[\"coverage_fraction\"].clip(0,1).tolist(), class_color)\n",
        "    fig.text(0.5, 0.05, f\"Detail Page | {page_num}\", ha='center', fontsize=9, color=SUBTLE_TEXT_COLOR)\n",
        "    return fig\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: MAIN GENERATOR FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_report(refs, output_dir=\"world_class_report_v2\", file_prefix=\"perception\", **summary_kwargs):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    df  = compute_metrics(refs)\n",
        "    agg = class_aggregates(df)\n",
        "    pdf_path = os.path.join(output_dir, f\"{file_prefix}_report.pdf\")\n",
        "    with PdfPages(pdf_path) as pdf:\n",
        "        fig = plot_summary_page_v2(df, agg, **summary_kwargs)\n",
        "        pdf.savefig(fig, facecolor=fig.get_facecolor()); plt.close(fig)\n",
        "        for i, cid in enumerate(agg[\"class_id\"]):\n",
        "            d = df[df[\"class_id\"]==cid].copy()\n",
        "            a = agg[agg[\"class_id\"]==cid].iloc[0]\n",
        "            fig = plot_class_page_letter_v2(d, a, page_num=i+2)\n",
        "            pdf.savefig(fig, facecolor=fig.get_facecolor()); plt.close(fig)\n",
        "    print(\"✨ Report generated\")\n",
        "    print(\"📄\", pdf_path)\n",
        "    return pdf_path"
      ],
      "metadata": {
        "id": "_cTLivekgGwF"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "# --- Report Statistics and Core Assumptions (for Page 2) ---\n",
        "report_data = {\n",
        "    'domain': [\n",
        "        'Agency & Control', 'Physical Body', 'Temporal Causality',\n",
        "        'Multisensory', 'Mathematical Cognition', 'Symbolic & Abstract'\n",
        "    ],\n",
        "    'mean_coverage': [0.52, 0.30, 0.28, 0.27, 0.16, 0.13],\n",
        "    'assumption': [\n",
        "        \"Human agency relies on immediate, low-latency feedback. Augmentation must preserve this temporal link to feel natural and intuitive.\",\n",
        "        \"Our native senses perceive only narrow bands of a much wider physical reality. Technology must act as a translator for this unseen information.\",\n",
        "        \"We perceive cause-and-effect relationships primarily within short time windows. AI is needed to reveal complex, long-range causal patterns.\",\n",
        "        \"The brain's method of combining sensory inputs is variable and imperfect. Augmentation can stabilize and enhance sensory integration, especially in XR.\",\n",
        "        \"Core numerical and logical abilities are highly constrained. The primary demand is for cognitive offloading tools that augment working memory.\",\n",
        "        \"The human ability to distinguish between abstract symbols like sounds or concepts is coarse. Specialized tools are needed for finer-grained analysis.\"\n",
        "    ]\n",
        "}\n",
        "df_report = pd.DataFrame(report_data).sort_values(by='mean_coverage', ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def compute_metrics(refs):\n",
        "    df = refs.copy()\n",
        "    df = df[df.apply(lambda r: \"Hmin\" in r and \"Hmax\" in r and r[\"Hmin\"] < r[\"Hmax\"] and r[\"Umin\"] < r[\"Umax\"], axis=1)].copy()\n",
        "    def span_fraction(r):\n",
        "        if r[\"scale\"] == \"log\":\n",
        "            if any(x <= 0 for x in [r[\"Umin\"], r[\"Umax\"], r[\"Hmin\"], r[\"Hmax\"]]): return 0.0\n",
        "            return (math.log10(r[\"Hmax\"]) - math.log10(r[\"Hmin\"])) / (math.log10(r[\"Umax\"]) - math.log10(r[\"Umin\"]))\n",
        "        return (r[\"Hmax\"] - r[\"Hmin\"]) / (r[\"Umax\"] - r[\"Umin\"])\n",
        "    def u_span(r):\n",
        "        if r[\"scale\"] == \"log\":\n",
        "            if r[\"Umin\"] <= 0 or r[\"Umax\"] <= 0: return 0.0\n",
        "            return math.log10(r[\"Umax\"]) - math.log10(r[\"Umin\"])\n",
        "        return r[\"Umax\"] - r[\"Umin\"]\n",
        "    df[\"coverage_fraction\"] = df.apply(span_fraction, axis=1)\n",
        "    df[\"U_weight\"] = df.apply(u_span, axis=1)\n",
        "    tot = df[\"U_weight\"].sum()\n",
        "    df[\"U_share\"] = df[\"U_weight\"] / tot if tot > 0 else 0.0\n",
        "    return df\n",
        "\n",
        "def class_aggregates(df):\n",
        "    base = df.groupby(\"class_id\")[\"coverage_fraction\"].agg(\n",
        "        coverage_mean=\"mean\", median=\"median\", min=\"min\", max=\"max\", n_aspects=\"count\"\n",
        "    ).reset_index()\n",
        "    base[\"range\"] = base[\"max\"] - base[\"min\"]\n",
        "    # The 'wmean' part isn't strictly necessary for the fix, but this is the full original function\n",
        "    wmean = (df.groupby(\"class_id\")\n",
        "             .apply(lambda x: np.average(x[\"coverage_fraction\"], weights=x.get(\"U_weight\", np.ones(len(x)))))\n",
        "             .reset_index(name=\"coverage_mean_weighted\"))\n",
        "    return base.merge(wmean, on=\"class_id\", how=\"left\")\n",
        "\n",
        "def _class_order_by_pc1(agg):\n",
        "    X = agg[[\"coverage_mean\", \"n_aspects\"]].astype(float).to_numpy()\n",
        "    X = (X - X.mean(0)) / np.where(X.std(0) == 0, 1, X.std(0))\n",
        "    if X.shape[1] > 0:\n",
        "        _, _, vt = np.linalg.svd(np.nan_to_num(X), full_matrices=False)\n",
        "        pc1 = (X @ vt.T)[:, 0]\n",
        "        return agg[\"class_id\"].to_numpy()[np.argsort(pc1)]\n",
        "    return agg[\"class_id\"].to_numpy()\n",
        "\n",
        "def _base_positions(agg):\n",
        "    classes = _class_order_by_pc1(agg)\n",
        "    cx = np.linspace(0.06, 0.94, len(classes))\n",
        "    return classes, dict(zip(classes, cx)), (cx[1]-cx[0] if len(cx)>1 else 0.2)\n",
        "\n",
        "def _continuous_intensity_for_cover(df, agg, width=4200, sigma_mul=0.22, jitter_mul=0.18, gamma=0.8, seed=42):\n",
        "    classes, pos, spacing = _base_positions(agg)\n",
        "    x = np.linspace(0, 1, width); I = np.zeros_like(x)\n",
        "    rng = np.random.default_rng(seed); sigma = max(1e-3, spacing * sigma_mul)\n",
        "    for _, r in df.iterrows():\n",
        "        cov = float(np.clip(r[\"coverage_fraction\"], 0, 1))\n",
        "        cx  = pos.get(r[\"class_id\"], 0.5)\n",
        "        xi  = np.clip(cx + rng.normal(0, spacing * jitter_mul), 0, 1)\n",
        "        I  += cov * np.exp(-0.5 * ((x - xi) / sigma)**2)\n",
        "    if I.max() > 0: I /= I.max()\n",
        "    return I**gamma, classes, pos\n",
        "\n",
        "def plot_coverage_field(ax, df, agg):\n",
        "    \"\"\"NEW: A self-contained function to draw the visualization on a given Axes.\"\"\"\n",
        "    I, classes, pos = _continuous_intensity_for_cover(df, agg)\n",
        "    y = np.linspace(0, 1, 100)\n",
        "    vtex = 1 - 0.10 * np.cos(2 * np.pi * y) - 0.05 * np.cos(6 * np.pi * y)\n",
        "    img = np.clip(np.outer(vtex, I), 0, 1)\n",
        "    ax.imshow(img, cmap=\"Blues\", origin=\"lower\", aspect=\"auto\", extent=(0, 1, 0, 1))\n",
        "    ax.axis(\"off\")\n",
        "    for c in classes:\n",
        "        ax.plot([pos[c], pos[c]], [0.14, 0.88], color=(0,0,0,0.22), lw=0.55)\n",
        "        label = c.replace('_', ' ').replace(' ', '\\n') # Simple wrap\n",
        "        ax.text(pos[c], 0.08, label, ha=\"center\", va=\"top\", fontsize=7, color=\"#333333\")\n",
        "\n",
        "\n",
        "def create_letter_page():\n",
        "    fig = plt.figure(figsize=(8.5, 11), facecolor='white')\n",
        "    plt.rcParams['font.family'] = 'sans-serif'; plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "    plt.rcParams['text.color'] = '#111111'\n",
        "    return fig\n",
        "\n",
        "def draw_footer(fig, page_num, total_pages):\n",
        "    fig.text(0.5, 0.03, f\"A Quantitative Survey of Human Perceptual Limits | Page {page_num} of {total_pages}\",\n",
        "             ha='center', va='center', fontsize=8, color='#888888')\n",
        "\n",
        "def parse_citation(text, aspect_label):\n",
        "    url_match = re.search(r'(https?://[^\\s]+)', text)\n",
        "    url = url_match.group(1) if url_match else None\n",
        "    clean_text = re.sub(r'\\s*https?://[^\\s]+', '', text).strip()\n",
        "    if not clean_text.endswith('.'): clean_text += '.'\n",
        "    return f\"[{aspect_label}] {clean_text}\", url\n",
        "\n",
        "\n",
        "def create_page_one(page_num, total_pages, df_full, agg_full):\n",
        "    \"\"\"Creates the title page, abstract, intro, AND the new visualization.\"\"\"\n",
        "    fig = create_letter_page()\n",
        "\n",
        "    # --- Headers  ---\n",
        "    fig.text(0.5, 0.88, \"A Quantitative Survey of Human Perceptual Limits:\", ha='center', fontsize=20, weight='bold')\n",
        "    fig.text(0.5, 0.84, \"The Strategic Imperative for Augmentation\", ha='center', fontsize=18)\n",
        "    fig.text(0.5, 0.79, \"Dan Ehlers (pinballsurgeon@gmail.com) | August 31, 2025\", ha='center', fontsize=10, color='#666666')\n",
        "\n",
        "    # --- Abstract  ---\n",
        "    fig.text(0.12, 0.72, \"Abstract\", fontsize=12, weight='bold')\n",
        "    abstract_text = \"This report provides a quantitative survey of human perceptual and cognitive capabilities, framed as the 'coverage' of universally available spectra. By synthesizing data across six core domains—from basic sensory input to abstract mathematical thought—we map the narrow windows through which humans experience reality. Our analysis reveals profound limitations, particularly in symbolic and mathematical reasoning. These findings establish a data-driven framework for identifying and prioritizing the critical human augmentation technologies required to meet the demands of 2026 and beyond.\"\n",
        "    fig.text(0.12, 0.68, textwrap.fill(abstract_text, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "\n",
        "    # --- Introduction ---\n",
        "    fig.text(0.12, 0.53, \"1. Introduction\", fontsize=12, weight='bold')\n",
        "    intro_text = \"The modern world generates information at a scale and complexity that far exceeds the natural processing limits of the human mind. The field of human augmentation aims to bridge this gap, using technology to enhance our innate abilities. However, to be effective, these efforts must be directed at our most significant limitations. This paper provides a clear, quantitative map of those limitations.\\n\\nWe introduce a standardized metric, 'perceptual coverage,' to measure the fraction of a universal physical or conceptual range that is accessible to human perception. By applying this metric across diverse functions, we create a comparative overview of our capabilities. This analysis serves a strategic purpose: to highlight the domains with the lowest coverage, thereby revealing where technological augmentation can provide the greatest impact.\"\n",
        "    fig.text(0.12, 0.49, textwrap.fill(intro_text, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "\n",
        "\n",
        "    fig.text(0.13, 0.28, \"Figure 1: Continuous Coverage Field (Equalized by Class)\", fontsize=11)\n",
        "    ax_field = fig.add_axes([0.12, 0.13, 0.76, 0.13])\n",
        "    plot_coverage_field(ax_field, df_full, agg_full)\n",
        "\n",
        "    draw_footer(fig, page_num, total_pages)\n",
        "    return fig\n",
        "\n",
        "def create_page_two(page_num, total_pages):\n",
        "    \"\"\"Creates the methodology, findings chart, and summary table.\"\"\"\n",
        "    fig = create_letter_page()\n",
        "    fig.text(0.12, 0.92, \"2. Method and Key Findings\", fontsize=14, weight='bold')\n",
        "    fig.text(0.12, 0.87, \"Methodology\", fontsize=11, weight='bold')\n",
        "    method_text = \"We evaluated over a dozen distinct aspects of perception and cognition, grouped into six domains. For each aspect, a 'human range' (H) was compared against a 'universal spectrum' (U) derived from scientific literature. Coverage was calculated as the ratio of the human span to the universal span. Spans for logarithmic scales (e.g., sound frequency) were calculated in log space to properly reflect perceptual sensitivity.\"\n",
        "    fig.text(0.12, 0.83, textwrap.fill(method_text, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "    fig.text(0.5, 0.72, \"Mean Perceptual Coverage by Domain\", ha='right', fontsize=10, weight='bold')\n",
        "    ax = fig.add_axes([0.3, 0.6, 0.6, 0.12])\n",
        "    bars = ax.barh(df_report['domain'], df_report['mean_coverage'], color='#3B5B8F', height=0.7)\n",
        "    ax.invert_yaxis(); ax.set_xlim(0, 0.6); ax.tick_params(axis='y', length=0); ax.tick_params(axis='x', labelsize=8, colors='#555555')\n",
        "    ax.spines[['top', 'right', 'left']].set_visible(False); ax.spines['bottom'].set_color('#AAAAAA')\n",
        "    ax.get_yaxis().set_ticks([]); ax.set_xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "    for i, (domain, value) in enumerate(zip(df_report['domain'], df_report['mean_coverage'])):\n",
        "        ax.text(-0.01, i, domain, va='center', ha='right', fontsize=9, weight='bold')\n",
        "        ax.text(value + 0.01, i, f\"{value*100:.0f}%\", va='center', ha='left', fontsize=9, weight='bold', color='#3B5B8F')\n",
        "    fig.text(0.5, 0.52, \"Table 1: Domain Coverage and Core Augmentation Assumptions\", ha='center', fontsize=10, weight='bold')\n",
        "    y_pos, row_h = 0.47, 0.08\n",
        "    fig.text(0.12, y_pos, \"Domain\", weight='bold', fontsize=10); fig.text(0.35, y_pos, \"Coverage\", weight='bold', fontsize=10)\n",
        "    fig.text(0.48, y_pos, \"Core Assumption for Augmentation\", weight='bold', fontsize=10)\n",
        "    fig.patches.extend([plt.Rectangle((0.12, y_pos - 0.02), 0.76, 0.002, fc='#AAAAAA', transform=fig.transFigure)])\n",
        "    for _, row in df_report.iterrows():\n",
        "        y_pos -= row_h\n",
        "        fig.text(0.12, y_pos, row['domain'], fontsize=9, va='top')\n",
        "        fig.text(0.35, y_pos, f\"{row['mean_coverage']:.0%}\", fontsize=9, va='top')\n",
        "        fig.text(0.48, y_pos, textwrap.fill(row['assumption'], 55), fontsize=9, va='top', linespacing=1.3)\n",
        "    draw_footer(fig, page_num, total_pages)\n",
        "    return fig\n",
        "\n",
        "# Page 3 and 4 functions remain unchanged...\n",
        "def create_page_three(page_num, total_pages):\n",
        "    fig = create_letter_page()\n",
        "    fig.text(0.12, 0.92, \"3. Strategic Augmentation Demands for 2026\", fontsize=14, weight='bold')\n",
        "    fig.text(0.12, 0.86, \"1. Sensory Translation and Transduction\", fontsize=11, weight='bold')\n",
        "    text1 = \"The profound limits in basic sensory perception (Physical Body domain) highlight an urgent need for technologies that make the invisible visible. The goal is to translate out-of-band energy—such as infrared, ultraviolet, or ultrasonic frequencies—into formats humans can intuitively understand. This moves beyond niche instruments to everyday tools for navigating the world.\"\n",
        "    fig.text(0.12, 0.82, textwrap.fill(text1, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "    fig.text(0.12, 0.72, \"2. Seamless Cognitive Offloading\", fontsize=11, weight='bold')\n",
        "    text2 = \"The critical bottlenecks in mathematical and symbolic thought demand AI-powered cognitive partners. These systems must augment working memory and automate complex reasoning, integrating smoothly with human thought processes rather than simply acting as calculators. The aim is to reduce cognitive load and enable focus on higher-level problem-solving.\"\n",
        "    fig.text(0.12, 0.68, textwrap.fill(text2, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "    fig.text(0.12, 0.58, \"3. Latency-Aware Interface Design\", fontsize=11, weight='bold')\n",
        "    text3 = \"The high coverage in Agency & Control is a warning: our sense of control is fragile and depends on immediate feedback. As we interact through more complex technology (VR/AR, remote robotics), designers must aggressively minimize lag. Preserving this temporal link is a fundamental constraint for any augmentation that feels direct and intuitive.\"\n",
        "    fig.text(0.12, 0.54, textwrap.fill(text3, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "    fig.text(0.12, 0.44, \"4. Conclusion\", fontsize=12, weight='bold')\n",
        "    conclusion_text = \"By quantifying the limits of human perception, this work provides a data-driven map for a strategic approach to human augmentation. The clearest takeaway is that our greatest needs lie not in amplifying existing strengths, but in systematically bridging the vast gaps in our sensory and cognitive capabilities. The technologies designed to address these specific, measured shortfalls will define the next generation of human-computer partnership and unlock new potential for discovery and understanding.\"\n",
        "    fig.text(0.12, 0.40, textwrap.fill(conclusion_text, 90), ha='left', va='top', fontsize=10.5, linespacing=1.4)\n",
        "    draw_footer(fig, page_num, total_pages)\n",
        "    return fig\n",
        "\n",
        "def create_page_four_references(page_num, total_pages):\n",
        "    fig = create_letter_page()\n",
        "\n",
        "    fig.text(0.12, 0.92, \"5. References\", fontsize=14, weight='bold')\n",
        "\n",
        "    y_pos = 0.88\n",
        "    for index, row in refs_df.iterrows():\n",
        "        if y_pos < 0.1: break\n",
        "\n",
        "        citation_text, url = parse_citation(row['source_human'], row['aspect_label'])\n",
        "\n",
        "        wrapped_text = textwrap.fill(citation_text, 95)\n",
        "\n",
        "        text_obj = fig.text(0.12, y_pos, wrapped_text, ha='left', va='top', fontsize=8.5, linespacing=1.3)\n",
        "\n",
        "        if url:\n",
        "            text_obj.set_url(url)\n",
        "        num_lines = len(wrapped_text.split('\\n'))\n",
        "        y_pos -= (num_lines * 0.015 + 0.012)\n",
        "\n",
        "    draw_footer(fig, page_num, total_pages)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def generate_final_report(filename=\"Human_Augmentation_Report_2026.pdf\"):\n",
        "    \"\"\"Orchestrates the creation of all pages and saves them to a single PDF.\"\"\"\n",
        "\n",
        "    # --- MODIFIED: Prepare the full dataset needed for the visualization ---\n",
        "    print(\"🔬 Computing metrics from full dataset for visualization...\")\n",
        "    df_full = compute_metrics(refs_df)\n",
        "    agg_full = class_aggregates(df_full)\n",
        "\n",
        "    TOTAL_PAGES = 4\n",
        "    with PdfPages(filename) as pdf:\n",
        "        print(\"📄 Generating Page 1: Title, Intro, and Coverage Field...\")\n",
        "        # Pass the computed data to the page one function\n",
        "        fig1 = create_page_one(1, TOTAL_PAGES, df_full, agg_full)\n",
        "        pdf.savefig(fig1)\n",
        "        plt.close(fig1)\n",
        "\n",
        "        print(\"📄 Generating Page 2: Findings...\")\n",
        "        fig2 = create_page_two(2, TOTAL_PAGES)\n",
        "        pdf.savefig(fig2)\n",
        "        plt.close(fig2)\n",
        "\n",
        "        print(\"📄 Generating Page 3: Demands...\")\n",
        "        fig3 = create_page_three(3, TOTAL_PAGES)\n",
        "        pdf.savefig(fig3)\n",
        "        plt.close(fig3)\n",
        "\n",
        "        print(\"📄 Generating Page 4: References...\")\n",
        "        fig4 = create_page_four_references(4, TOTAL_PAGES)\n",
        "        pdf.savefig(fig4)\n",
        "        plt.close(fig4)\n",
        "\n",
        "    print(f\"\\n✅ Final report successfully generated. Saved as '{filename}'\")\n",
        "\n",
        "# --- Execute the report generation ---\n",
        "generate_final_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYO4fdgpOMyY",
        "outputId": "3c4b8810-ce7d-435a-e641-f12e169cc49d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔬 Computing metrics from full dataset for visualization...\n",
            "📄 Generating Page 1: Title, Intro, and Coverage Field...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-136351920.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: np.average(x[\"coverage_fraction\"], weights=x.get(\"U_weight\", np.ones(len(x)))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Generating Page 2: Findings...\n",
            "📄 Generating Page 3: Demands...\n",
            "📄 Generating Page 4: References...\n",
            "\n",
            "✅ Final report successfully generated. Saved as 'Human_Augmentation_Report_2026.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pypdf import PdfWriter\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "arxiv_style_report_path = \"Human_Augmentation_Report_2026.pdf\"\n",
        "\n",
        "details_report_dir = \"world_class_report_v2\"\n",
        "details_report_filename = \"perception_report.pdf\"\n",
        "details_report_path = os.path.join(details_report_dir, details_report_filename)\n",
        "\n",
        "final_combined_path = \"Perception_Report_Complete_Final.pdf\"\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "print(\"🚀 Starting the final report generation pipeline...\")\n",
        "\n",
        "try:\n",
        "\n",
        "    print(f\"📄 Generating the 4-page 'arXiv-style' report (front matter)...\")\n",
        "    generate_final_report(filename=arxiv_style_report_path)\n",
        "\n",
        "    print(f\"📄 Generating the detailed summary & breakdown report...\")\n",
        "    _ = generate_report(refs_df, output_dir=details_report_dir, file_prefix=\"perception\")\n",
        "    if not os.path.exists(arxiv_style_report_path) or not os.path.exists(details_report_path):\n",
        "        raise FileNotFoundError(\"One or both of the required PDF files were not generated successfully.\")\n",
        "\n",
        "    print(f\"\\n📎 Assembling the final document...\")\n",
        "    pdf_merger = PdfWriter()\n",
        "\n",
        "    print(f\"   + Adding Front Matter: '{arxiv_style_report_path}'\")\n",
        "    pdf_merger.append(arxiv_style_report_path)\n",
        "\n",
        "    print(f\"   + Adding Detailed Report: '{details_report_path}'\")\n",
        "    pdf_merger.append(details_report_path)\n",
        "\n",
        "    with open(final_combined_path, \"wb\") as output_pdf:\n",
        "        pdf_merger.write(output_pdf)\n",
        "\n",
        "    pdf_merger.close()\n",
        "    print(f\"\\n🎉 Success! The final combined report is complete.\")\n",
        "    print(f\"   Saved as: '{final_combined_path}'\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"❌ ERROR: A required variable or function is not defined. Please ensure all previous cells have been run.\")\n",
        "    print(f\"   Missing component: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: An unexpected error occurred during the finalization process.\")\n",
        "    print(f\"   Details: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LeolSMeOM2r",
        "outputId": "28fa2e99-a9a7-42f2-f3c5-753eef991158"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting the final report generation pipeline...\n",
            "📄 Generating the 4-page 'arXiv-style' report (front matter)...\n",
            "🔬 Computing metrics from full dataset for visualization...\n",
            "📄 Generating Page 1: Title, Intro, and Coverage Field...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-136351920.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: np.average(x[\"coverage_fraction\"], weights=x.get(\"U_weight\", np.ones(len(x)))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Generating Page 2: Findings...\n",
            "📄 Generating Page 3: Demands...\n",
            "📄 Generating Page 4: References...\n",
            "\n",
            "✅ Final report successfully generated. Saved as 'Human_Augmentation_Report_2026.pdf'\n",
            "📄 Generating the detailed summary & breakdown report...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-136351920.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: np.average(x[\"coverage_fraction\"], weights=x.get(\"U_weight\", np.ones(len(x)))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨ Report generated\n",
            "📄 world_class_report_v2/perception_report.pdf\n",
            "\n",
            "📎 Assembling the final document...\n",
            "   + Adding Front Matter: 'Human_Augmentation_Report_2026.pdf'\n",
            "   + Adding Detailed Report: 'world_class_report_v2/perception_report.pdf'\n",
            "\n",
            "🎉 Success! The final combined report is complete.\n",
            "   Saved as: 'Perception_Report_Complete_Final.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "xPPHLgWqOM6t"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUyTyb_R4WHV"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IOZCU5MOM_F"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ws6cQXgRONHn"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}